# üß† Sales Analytics Data Pipeline on GCP

This project demonstrates an **end-to-end real-time data pipeline** on Google Cloud Platform (GCP), built for **Sales Analytics**. It includes raw ingestion, transformation, and loading into BigQuery for real-time analysis.

---

## üìå Project Structure

```bash
SalesAnalytics/
‚îÇ
‚îú‚îÄ‚îÄ terraform/                   # Infrastructure as Code (Pub/Sub, BigQuery, IAM, etc.)
‚îÇ
‚îú‚îÄ‚îÄ dataflow/
‚îÇ   ‚îú‚îÄ‚îÄ raw_ingest/              # Ingests raw sales data from Pub/Sub ‚Üí BigQuery (sales_raw)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ raw_ingest.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ clean_transform/         # Batch cleaning: BigQuery (sales_raw) ‚Üí BigQuery (sales_cleaned)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clean_transform.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ stream_clean_transform/  # Real-time cleaning: Pub/Sub ‚Üí Dataflow ‚Üí BigQuery (sales_cleaned)
‚îÇ       ‚îî‚îÄ‚îÄ clean_streaming.py
‚îÇ
‚îú‚îÄ‚îÄ README.md                    # You're here!
‚îî‚îÄ‚îÄ .gitignore

1Ô∏è‚É£ Raw Ingestion (Streaming)

Reads sales data from Pub/Sub topic: retail-sales-stream

Stores raw JSON into BigQuery table: sales_data.sales_raw

Purpose: Store raw data for audit, replay, debugging

2Ô∏è‚É£ Clean Transform (Streaming)

Reads from same Pub/Sub topic

Applies transformation and cleaning:

Calculates total items per transaction

Computes average price per item

Writes to BigQuery table: sales_data.sales_cleaned

Used for real-time dashboards & analytics




Enable these APIs:

pubsub.googleapis.com

bigquery.googleapis.com

dataflow.googleapis.com

iam.googleapis.com



``` terraform init ```

``` terraform apply -var="project_id=your-project-id" -var="region=us-central1" ```



Test data simulator

> cd pubsub_simulator

> pip install -r requirements.txt

> gcloud auth application-default login (Set your GCP credentials (if not already using ADC):)

>
 ``` python simulate_transactions.py --project_id="your-gcp-project-id" --topic_id="retail-sales-stream" ```

_________!!!!_______
(Optional, for Production): Use the Terraform-created Service Account

You can also download the key file for the pubsub-simulator service account and authenticate like this:

export GOOGLE_APPLICATION_CREDENTIALS="/path/to/simulator-key.json"


We'll handle that when we automate more later.
___________________



TEST raw ingestion dataflow:

python raw_ingest.py \
  --runner=DirectRunner \
  --project_id=your-project-id \
  --input_topic=projects/your-project-id/topics/retail-sales-stream \
  --output_table=your-project-id:sales_data.sales_raw


python raw_ingest.py \
  --runner=DataflowRunner \
  --project_id=your-project-id \
  --region=us-central1 \
  --input_topic=projects/your-project-id/topics/retail-sales-stream \
  --output_table=your-project-id:sales_data.sales_raw \
  --temp_location=gs://your-bucket/tmp \
  --dead_letter_bucket=your-bucket


Test transform pipeline dataflow:

python clean_streaming.py \
  --runner=DirectRunner \
  --project_id=your-project-id \
  --input_topic=projects/your-project-id/topics/retail-sales-stream \
  --output_table=your-project-id:sales_data.sales_cleaned


python clean_transform.py \
  --runner=DataflowRunner \
  --project_id=your-project-id \
  --region=us-central1 \
  --output_table=your-project-id:sales_data.sales_cleaned \
  --temp_location=gs://your-bucket/tmp \
  --dead_letter_bucket=your-bucket


Security Best Practices

Use separate service accounts for pipelines with least-privilege access

Set IAM roles using Terraform (roles/pubsub.subscriber, roles/bigquery.dataEditor)

Ensure Cloud Audit Logs are enabled

Store secrets using Secret Manager (if needed)

‚ôªÔ∏è Scalability & Resilience

Pub/Sub ensures decoupled ingestion (auto-scales)

Dataflow auto-scales with load

BigQuery supports high-throughput streaming inserts

Raw layer ensures recovery and reprocessing

Optional dead-letter queues can be added

üõ†Ô∏è To Do / Enhancements

 Add Dead-letter bucket for malformed messages

 Add Airflow/Cloud Scheduler for batch job

 Add unit tests for transformations

 Add CI/CD for Terraform + Dataflow deployment

 Add Looker dashboards





    

   
