# ğŸ§  Sales Analytics Real-Time Data Pipeline on GCP

This project implements an **end-to-end real-time data pipeline** on **Google Cloud Platform (GCP)** for sales analytics use cases. It supports raw data ingestion, real-time transformation, and storage in BigQuery for analytics and dashboarding.

---

## ğŸ“¦ Features

- ğŸ”„ **Real-time streaming ingestion** using Pub/Sub
- ğŸ“¥ **Raw event storage** in BigQuery (`sales_raw`) for audit & replay
- ğŸ§¹ **Real-time transformation** via Dataflow & Apache Beam
- ğŸ“Š **Cleaned data** loaded into BigQuery (`sales_cleaned`)
- â˜ï¸ Fully automated provisioning using **Terraform**
- ğŸ”’ Secure and scalable architecture with best practices

---

## ğŸ§± Project Structure

```bash
SalesAnalytics/
â”‚
â”œâ”€â”€ terraform/                   # IaC for Pub/Sub, BigQuery, IAM roles, etc.
â”‚
â”œâ”€â”€ dataflow/
â”‚   â”œâ”€â”€ raw_ingest/              # Ingests raw data from Pub/Sub â†’ BQ (sales_raw)
â”‚   â”‚   â””â”€â”€ raw_ingest.py
â”‚   â”‚
â”‚   â”œâ”€â”€ clean_transform/         # Batch clean: BQ (sales_raw) â†’ BQ (sales_cleaned)
â”‚   â”‚   â””â”€â”€ clean_transform.py
â”‚   â”‚
â”‚   â””â”€â”€ stream_clean_transform/  # Real-time clean: Pub/Sub â†’ Dataflow â†’ BQ (sales_cleaned)
â”‚       â””â”€â”€ clean_streaming.py
â”‚
â”œâ”€â”€ pubsub_simulator/            # Simulates transaction data to Pub/Sub
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore



______________________________________________________________


ğŸ“ˆ Pipeline Overview
1ï¸âƒ£ Raw Ingestion (Streaming)

Source: Pub/Sub topic retail-sales-stream

Sink: BigQuery table sales_data.sales_raw

Purpose: Store unprocessed JSON events for auditing, replay, and recovery

2ï¸âƒ£ Real-Time Transformation (Streaming)

Source: Same Pub/Sub topic

Transformations:

Compute total items per transaction

Compute average price per item

Sink: BigQuery table sales_data.sales_cleaned

Purpose: Enables near real-time dashboarding & insights

3ï¸âƒ£ Batch Transformation (Optional)

Source: BigQuery sales_raw

Sink: BigQuery sales_cleaned

Use Case: Backfills, historical reprocessing

âš™ï¸ Setup Instructions
âœ… Prerequisites

Ensure the following GCP services are enabled:

pubsub.googleapis.com

bigquery.googleapis.com

dataflow.googleapis.com

iam.googleapis.com

Install required tools:

# Install Apache Beam
pip install apache-beam[gcp]

# Install Terraform
https://developer.hashicorp.com/terraform/install

ğŸš€ Step 1: Deploy Infrastructure with Terraform
cd terraform
terraform init
terraform apply -var="project_id=your-project-id" -var="region=us-central1"

ğŸ§ª Step 2: Test Data Generator (Pub/Sub Simulator)
cd pubsub_simulator
pip install -r requirements.txt

# Authenticate to GCP if not already
gcloud auth application-default login

# Send sample transactions
python simulate_transactions.py \
  --project_id="your-project-id" \
  --topic_id="retail-sales-stream"


âœ… Optional (for production):
Use the Terraform-created service account and authenticate:

export GOOGLE_APPLICATION_CREDENTIALS="/path/to/simulator-key.json"

ğŸ” Run Pipelines
ğŸ“¥ Raw Ingestion (from Pub/Sub â†’ BigQuery sales_raw)
â–¶ï¸ DirectRunner (Local)
python raw_ingest.py \
  --runner=DirectRunner \
  --project_id=your-project-id \
  --input_topic=projects/your-project-id/topics/retail-sales-stream \
  --output_table=your-project-id:sales_data.sales_raw

â˜ï¸ Dataflow Runner (Production)
python raw_ingest.py \
  --runner=DataflowRunner \
  --project_id=your-project-id \
  --region=us-central1 \
  --input_topic=projects/your-project-id/topics/retail-sales-stream \
  --output_table=your-project-id:sales_data.sales_raw \
  --temp_location=gs://your-bucket/tmp

ğŸ§¹ Real-Time Clean Transform (Pub/Sub â†’ Dataflow â†’ BigQuery sales_cleaned)
â–¶ï¸ DirectRunner
python clean_streaming.py \
  --runner=DirectRunner \
  --project_id=your-project-id \
  --input_topic=projects/your-project-id/topics/retail-sales-stream \
  --output_table=your-project-id:sales_data.sales_cleaned

â˜ï¸ Dataflow Runner
python clean_streaming.py \
  --runner=DataflowRunner \
  --project_id=your-project-id \
  --region=us-central1 \
  --input_topic=projects/your-project-id/topics/retail-sales-stream \
  --output_table=your-project-id:sales_data.sales_cleaned \
  --temp_location=gs://your-bucket/tmp

ğŸ§¼ Optional: Batch Cleaning Pipeline (sales_raw â†’ sales_cleaned)
python clean_transform.py \
  --runner=DataflowRunner \
  --project_id=your-project-id \
  --region=us-central1 \
  --output_table=your-project-id:sales_data.sales_cleaned \
  --temp_location=gs://your-bucket/tmp

ğŸ” Security Best Practices

ğŸ”’ Use separate service accounts with least-privilege access

âœ… Assign IAM roles via Terraform:

roles/pubsub.subscriber

roles/bigquery.dataEditor

ğŸ§¾ Enable Cloud Audit Logs

ğŸ”‘ Use Secret Manager for secure credentials (if needed)

â™»ï¸ Scalability & Reliability
Component	Benefit
Pub/Sub	Decouples ingestion, auto-scales with message volume
Dataflow	Fully managed streaming engine, handles load spikes
BigQuery	Optimized for high-speed streaming inserts
Raw Layer	Ensures reprocessing, debugging, and long-term storage
Dead-letter handling	(Planned) Capture failed records for future inspection
ğŸ“Š Optional: Real-Time Dashboard

Use Looker Studio
 or any BI tool to visualize data from sales_data.sales_cleaned. Build dashboards showing:

Sales volume by store or time

Items per transaction

Average spend per customer

Real-time KPIs

ğŸ› ï¸ To-Do / Enhancements

 Add dead-letter buckets for failed records

 Schedule batch jobs using Cloud Scheduler / Airflow

 Add unit tests for transformation logic

 Implement CI/CD with GitHub Actions

 Add Looker dashboards for stakeholders
